# MoE Architecture
use_moe: false
moe_n_experts: 8
moe_top_k: 2
moe_n_hidden: 1024
moe_activation: "swiGLU"
moe_use_shared: false
moe_n_layers: ${model.n_layer}

moe_router_noise: 0.005
moe_router_temp: 1.0
moe_lb_coef: 0.01

# Optional schedules for MoE hyperparameters (tokens-based)
# Schedules: 'none' | 'linear' | 'cosine'
moe_router_temp_schedule: none
moe_router_temp_start: ${model.moe_router_temp}
moe_router_temp_end: 1.0

moe_router_noise_schedule: none
moe_router_noise_start: ${model.moe_router_noise}
moe_router_noise_end: 0.0

moe_lb_coef_schedule: none
moe_lb_coef_start: ${model.moe_lb_coef}
moe_lb_coef_end: ${model.moe_lb_coef}
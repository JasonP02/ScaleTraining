## Model and training knobs

# Runtime
device: cuda
batch_size: 32
accum_steps: 4
grad_clip_norm: 1.0
max_train_tokens: 40000000
max_val_tokens: 1000000000
logits_chunk_size: 256
use_flash_sdp: true
use_mem_efficient_sdp: true
use_math_sdp: false

# Early stopping
early_stop_tokens_without_improvement: 2000000
early_stop_min_delta: 0.0005

# Transformer architecture
vocab_size: 50257
n_layer: 5
max_seq_len: 2048
n_head: 8
n_embed: 512
n_hidden: 2048
bias: true
UE_bias: false
attn_dropout: 0.2
resid_dropout: 0.2
use_checkpoint: true
use_rope: true
rope_config:
  theta: 10000
  use_complex: true

# Evaluation
eval_datasets:
  - openai/gsm8k
  - Sadanto3933/ai2_arc
eval_interval_tokens: 0
eval_max_batches: 50
eval_batch_size: 1

# Optimizer
lr: 0.02
muon_lr: 0.02
beta: 0.9
beta2: 0.999
weight_decay: 0.0
ns_iters: 5
eps: 1e-8
primary_optimizer: muon
use_baseline_adam: false
baseline_adam_config:
  lr: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]

# LR scheduling
lr_schedule: cosine
warmup_tokens: 1000000
min_lr_scale: 0.1

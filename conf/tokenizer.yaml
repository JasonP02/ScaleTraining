# Tokenization, dataset selection, and packing knobs.

hf_dataset_names:
dataset_tag: ""
strict_dataset_compat: false

# Tokenization behavior
tokenizer_name: meta-llama/Llama-2-13b-hf
tokenizer_type: hf
num_proc: 8

# Packing behavior
do_packing: false
pack_num_proc: 8
pack_map_batch_size: 400
pack_writer_batch_size: 4000

# Dataset-specific tokenizer training
tokenizer_vocab_size: 12800

## Training runtime and token budget knobs

device: cuda
batch_size: 16
accum_steps: 4
grad_clip_norm: 1.0
max_train_tokens: 40000000
max_val_tokens: 1000000000
logits_chunk_size: 256
use_flash_sdp: true
use_mem_efficient_sdp: true
use_math_sdp: false

# Early stopping
early_stop_tokens_without_improvement: 2000000
early_stop_min_delta: 0.0005

# Evaluation cadence (used during training and optional loaders)
eval_interval_tokens: 0
eval_max_batches: 50
eval_batch_size: 1

# DataLoader knobs (used during training)
loader_num_workers: 4
loader_pin_memory: true
loader_persistent_workers: true
loader_prefetch_factor: 2

# About
This project provides a harness for training a dense or MoE transformer on a single GPU. It is a personal project for experimenting with LLM training.

# Usage
The project uses hydra for configuration management

The configuration options include:

- Grid sweeping for testing different hyperparameter configurations:
- Logging for weights and biases, model output 
## Entrypoints
``` train.py ```


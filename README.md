# ScaleTrain: A repository for scaling up LLM training

This is a personal project where I start with a simple baseline: train a 10m transformer on tinystories, and scale up parameters so I can train a 7B model.

Status: implementing decoder only transformer architecture for tinystories, and training
